# ğŸ§  Doctor AI â€” Multi-Modal Medical Assistant

An intelligent **multi-modal AI system** that mimics how a real doctor communicates. Users can **speak** their symptoms and **upload an image**, and the bot will analyze the input using advanced **LLMs**, **vision models**, and **TTS synthesis** to deliver a **human-like medical voice response**.

## ğŸš€ Features

- ğŸ¤ **Voice Input**: Speak symptoms directly to the system
- ğŸ–¼ï¸ **Image Upload**: Upload medical images (e.g., rashes, X-rays)
- ğŸ§  **LLM + Vision Analysis**: Uses GROQ's LLaMA-3 Vision model for intelligent reasoning
- ğŸ”Š **Text-to-Speech Response**: Responds with a lifelike medical voice using TTS
- ğŸ”„ **Multi-modal Fusion**: Combines voice and visual context for a better diagnosis

---
## ğŸ¥ Video Explanation

Hereâ€™s a brief video walkthrough of the **Doctor AI â€“ Multi-Modal Medical Assistant**:

[![Doctor AI â€“ Multi-Modal Medical Assistant Video](https://img.youtube.com/vi/mYvFn3bruvA/0.jpg)](https://www.youtube.com/watch?v=mYvFn3bruvA)

Click the image above to watch the full video!

---

## ğŸŸ© Screenshots of responses(Text+Audio) :
![image](https://github.com/user-attachments/assets/6ef3b156-24f1-46a5-b098-5184deb7ca86)

![image](https://github.com/user-attachments/assets/eabb3021-73d6-4255-bf56-d1f35de3d69a)

![image](https://github.com/user-attachments/assets/f22020c0-77dd-42eb-87e5-b3f5cf530c5b)

---
## ğŸ§  Tech Stack Breakdown

A detailed view of all the technologies used in each phase of the AI Doctor Bot pipeline:

---

### ğŸŸ© Phase 1 â€“ Audio Recorder  
**Purpose:** Capture user voice input  

**Tech Stack:**
- **Gradio** â€“ Interactive UI for microphone input  
- **Python** â€“ Backend logic and orchestration  
- **Gradio.Audio** â€“ Component to capture real-time audio  
- **WAV/MP3 File Handling** â€“ Temporarily saves recorded audio  

---

### ğŸŸ¦ Phase 2 â€“ Speech-to-Text (STT)  
**Purpose:** Convert userâ€™s spoken input into transcribed text  

**Tech Stack:**
- **Groq API** â€“ Provides fast inference for AI models  
- **OpenAI Whisper (via Groq)** â€“ Powerful STT AI model (`whisper-large-v3`)  
- **Python** â€“ Handles API interaction and file operations  
- **dotenv (.env)** â€“ Manages and secures API keys  

---

### ğŸŸ¨ Phase 3 â€“ Vision + LLM Response  
**Purpose:** Analyze uploaded medical images and generate diagnostic responses  

**Tech Stack:**
- **Meta's LLaMA Vision-Language Model** â€“ Performs multimodal analysis (text + image)  
- **Groq API** â€“ Delivers high-speed, low-latency LLM inference  
- **Base64 Encoding** â€“ Prepares image data for processing  
- **Gradio.Image** â€“ Handles image file uploads via UI  
- **Prompt Engineering** â€“ Custom instructions to mimic a doctorâ€™s tone  

---

### ğŸŸ¥ Phase 4 â€“ Text-to-Speech (TTS)  
**Purpose:** Convert AI-generated diagnosis into voice output  

**Tech Stack:**
- **ElevenLabs API** â€“ Converts text into high-quality, human-like speech  
- **Python Requests** â€“ Sends LLM output to ElevenLabs and fetches audio  
- **MP3 File Output** â€“ Saves and returns audio response  
- **Gradio.Audio** â€“ Plays back audio in the web interface  

---


## ğŸ§ªExample Flow

1. User clicks **mic** and speaks: â€œI have pain near my lower back.â€
2. User uploads an image of the affected area.
3. AI processes voice + image.
4. Output:
   - **Text diagnosis** appears in chat.
   - **Audio response** plays automatically.


---
# Guide to Installation

1. Clone the repository:
   
     git clone https://github.com/ayusharyan2704/Doctor-AI-Multi-Modal-Medical-Assistant.git
   
     cd Doctor-AI-Multi-Modal-Medical-Assistant

2. Create and activate a virtual environment:
     ## On Windows
     python -m venv venv
   
     venv\Scripts\activate
   
     ## On Mac/Linux
     python -m venv venv
   
     source venv/bin/activate

4. Install the required dependencies:

   pip install -r requirements.txt

6. Configure Environment Variables:

   Create a .env file in the root directory.

   Add your keys:
   
     GROQ_API_KEY=your_groq_api_key
   
     ELEVENLABS_API_KEY=your_elevenlabs_api_key
   
7. Usage : Once the setup is complete, run python final.py

---



